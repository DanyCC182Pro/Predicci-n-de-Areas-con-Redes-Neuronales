{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHDhMWYd0a1cKYw2qlDFyo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanyCC182Pro/Predicci-n-de-Areas-con-Redes-Neuronales/blob/main/Predicci%C3%B3n%20de%20Areas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **El método de Gauss**\n",
        "El método de Gauss para calcular áreas de polígonos, también conocido como Fórmula del Área de Gauss o Algoritmo de la Lazada, es un algoritmo matemático utilizado para determinar el área de un polígono simple cuyos vértices están especificados por pares de coordenadas en un plano. Este método es particularmente útil cuando se necesita calcular el área de polígonos irregulares o cuando se trabaja con coordenadas en un sistema de coordenadas cartesianas.\n",
        "\n",
        "La fórmula se basa en el cruce de productos de las coordenadas correspondientes de cada par de vértices, similar a cómo se ataría una lazada. Este proceso implica tomar la primera coordenada (x) de un vértice y multiplicarla por la segunda coordenada (y) del siguiente vértice, luego repetir este proceso para todos los vértices del polígono, alternando entre los productos cruzados de las coordenadas (x) e (y).\n",
        "\n",
        "Matemáticamente, la fórmula se puede expresar como sigue:\n",
        "\n",
        "$ \\text{Área} = \\frac{1}{2} \\left| \\sum_{i=0}^{n-1} (x_i y_{i+1} - x_{i+1} y_i) \\right| $\n",
        "\n",
        "Donde $(x_i)$ y $(y_i)$ representan las coordenadas de los vértices del polígono, y la suma se realiza sobre todos los vértices, con el último vértice $(x_n)$ y $(y_n)$ conectándose al primero $(x_0)$ y $(y_0)$ para cerrar el polígono. La expresión dentro de los paréntesis calcula el valor absoluto de la suma de productos cruzados de las coordenadas (x) e (y) de cada par de vértices adyacentes, y luego se divide por 2 para obtener el área."
      ],
      "metadata": {
        "id": "KOsyd2_HtZeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        num_vertices = int(input(\"Ingrese la cantidad de vértices del polígono: \"))\n",
        "        if num_vertices < 3:\n",
        "            print(\"El número de vértices debe ser igual o superior a 3.\")\n",
        "        else:\n",
        "            vertices = ingresar_vertices(num_vertices)\n",
        "            vertices = ordenar_sentido_contrario(vertices)\n",
        "            calcular_poligono(vertices)\n",
        "    except ValueError:\n",
        "        print(\"Entrada inválida. Por favor, ingrese un número válido.\")\n",
        "\n",
        "def ordenar_sentido_contrario(vertices):\n",
        "\n",
        "    ref_vertex = min(vertices, key=lambda x: (x[1], x[0]))\n",
        "\n",
        "    vertices.sort(key=lambda x: (np.arctan2(x[1] - ref_vertex[1], x[0] - ref_vertex[0]), -x[1], x[0]))\n",
        "    return vertices\n",
        "\n",
        "def ingresar_vertices(num_vertices):\n",
        "    vertices = []\n",
        "    for i in range(num_vertices):\n",
        "        while True:\n",
        "            try:\n",
        "                print(\"Ingrese las coordenadas del vértice\", i + 1, \"en el formato 'x y': \")\n",
        "                x, y = map(float, input().split())\n",
        "                vertices.append((x, y))\n",
        "                break\n",
        "            except ValueError:\n",
        "                print(\"Entrada inválida. Por favor, ingrese dos números separados por un espacio.\")\n",
        "    return vertices\n",
        "\n",
        "def calcular_poligono(vertices):\n",
        "    area = 0\n",
        "    for i in range(len(vertices)):\n",
        "        x1, y1 = vertices[i]\n",
        "        x2, y2 = vertices[(i + 1) % len(vertices)]\n",
        "        area += x1 * y2 - x2 * y1\n",
        "    area = abs(area) / 2\n",
        "    mostrar_resultado(area, len(vertices))\n",
        "\n",
        "def mostrar_resultado(area, num_vertices):\n",
        "    print(\"El área del polígono con\", num_vertices, \"vértices es:\", area)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HpVZdBmu-7w",
        "outputId": "c99a0171-cb71-4d5e-c495-7e5bab093772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese la cantidad de vértices del polígono: 4\n",
            "Ingrese las coordenadas del vértice 1 en el formato 'x y': \n",
            "1 2\n",
            "Ingrese las coordenadas del vértice 2 en el formato 'x y': \n",
            "3 4\n",
            "Ingrese las coordenadas del vértice 3 en el formato 'x y': \n",
            "2 0\n",
            "Ingrese las coordenadas del vértice 4 en el formato 'x y': \n",
            "9 -4.5\n",
            "El área del polígono con 4 vértices es: 19.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si el algoritmo para calcular el área de un polígono utilizando el método de Gauss resulta en cero, esto podría tener varias implicaciones dependiendo del contexto específico en el que se utilice el algoritmo. Aquí hay algunas posibles interpretaciones y consideraciones:\n",
        "\n",
        "1. Polígono Perfectamente Centralizado: Si el polígono está perfectamente centralizado en relación con el origen de las coordenadas (0,0), entonces el área calculada podría ser exactamente cero. Esto se debe a que el cruce de productos de las coordenadas (x) e (y) de los vértices se cancelará mutuamente, sumando cero en total. En este caso, el resultado de cero indica que el polígono no ocupa ningún espacio en el plano.\n",
        "\n",
        "2. Error de Entrada o Implementación: Un resultado de cero también podría indicar un error en la entrada de datos o en la implementación del algoritmo. Por ejemplo, si todos los vértices tienen las mismas coordenadas (es decir, son puntos idénticos), el resultado sería cero porque el área de un polígono con vértices coincidentes es efectivamente cero. De manera similar, un error en la implementación del algoritmo podría llevar a resultados incorrectos, incluido un cero cuando debería haber otro valor.\n",
        "\n",
        "3. Si los vértices de un polígono forman una recta plana, el área del polígono será cero. Esto se debe a que un conjunto de puntos que forman una línea recta no encierra ningún espacio en el plano; en otras palabras, no hay un área interior definida por esa línea. Matemáticamente, esto se refleja en el hecho de que el cruce de productos de las coordenadas (x) e (y) de los vértices en una línea recta siempre resultará en cero, ya que los productos cruzados de las mismas coordenadas se cancelarán entre sí."
      ],
      "metadata": {
        "id": "R0xghld7vdRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        num_vertices = int(input(\"Ingrese la cantidad de vértices del polígono: \"))\n",
        "        if num_vertices < 3:\n",
        "            print(\"El número de vértices debe ser igual o superior a 3.\")\n",
        "        else:\n",
        "            vertices = ingresar_vertices(num_vertices)\n",
        "            vertices = ordenar_sentido_contrario(vertices)\n",
        "            calcular_poligono(vertices)\n",
        "    except ValueError:\n",
        "        print(\"Entrada inválida. Por favor, ingrese un número válido.\")\n",
        "\n",
        "def ordenar_sentido_contrario(vertices):\n",
        "\n",
        "    ref_vertex = min(vertices, key=lambda x: (x[1], x[0]))\n",
        "\n",
        "    vertices.sort(key=lambda x: (np.arctan2(x[1] - ref_vertex[1], x[0] - ref_vertex[0]), -x[1], x[0]))\n",
        "    return vertices\n",
        "\n",
        "def ingresar_vertices(num_vertices):\n",
        "    vertices = []\n",
        "    for i in range(num_vertices):\n",
        "        while True:\n",
        "            try:\n",
        "                print(\"Ingrese las coordenadas del vértice\", i + 1, \"en el formato 'x y': \")\n",
        "                x, y = map(float, input().split())\n",
        "                vertices.append((x, y))\n",
        "                break\n",
        "            except ValueError:\n",
        "                print(\"Entrada inválida. Por favor, ingrese dos números separados por un espacio.\")\n",
        "    return vertices\n",
        "\n",
        "def calcular_poligono(vertices):\n",
        "    area = 0\n",
        "    for i in range(len(vertices)):\n",
        "        x1, y1 = vertices[i]\n",
        "        x2, y2 = vertices[(i + 1) % len(vertices)]\n",
        "        area += x1 * y2 - x2 * y1\n",
        "    area = abs(area) / 2\n",
        "    mostrar_resultado(area, len(vertices))\n",
        "\n",
        "def mostrar_resultado(area, num_vertices):\n",
        "    print(\"El área del polígono con\", num_vertices, \"vértices es:\", area)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brG_OTK1vXPj",
        "outputId": "9c232a24-8fe0-4a8a-855c-1a9bc3549236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese la cantidad de vértices del polígono: 3\n",
            "Ingrese las coordenadas del vértice 1 en el formato 'x y': \n",
            "1 1\n",
            "Ingrese las coordenadas del vértice 2 en el formato 'x y': \n",
            "2 2\n",
            "Ingrese las coordenadas del vértice 3 en el formato 'x y': \n",
            "3 3\n",
            "El área del polígono con 3 vértices es: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funcionamiento del Código**\n",
        "* Función ingresar_vertices: Esta función solicita al usuario que ingrese las coordenadas de los vértices del polígono. Cada vértice se representa como una tupla (x, y) y se recopilan todas estas tuplas en una lista llamada vertices. La entrada esperada es un número entero num_vertices que indica cuántos vértices tiene el polígono. Para cada vértice, el programa pide al usuario que introduzca su coordenada x e y.\n",
        "\n",
        "* Función ordenar_sentido_contrario: Antes de calcular el área del polígono, es necesario asegurarse de que los vértices estén ordenados en sentido contrario a las manecillas del reloj. Esto se hace mediante esta función. Primero, se encuentra el vértice de referencia *(ref_vertex)* que es el vértice más bajo y más a la izquierda. Luego, se ordenan todos los vértices según tres criterios:\n",
        "\n",
        "  1. El ángulo formado por el vértice actual y el vértice de referencia con el eje horizontal, calculado usando *np.arctan2(y - ref_vertex[1], x - ref_vertex[0]).*\n",
        "  2. La coordenada y del vértice, pero multiplicada por -1 para invertir el orden descendente a ascendente.\n",
        "  3. La coordenada x del vértice.\n",
        "\n",
        "Esto asegura que los vértices estén ordenados de tal manera que si se traza una línea entre cada par consecutivo de vértices, el resultado será un polígono cuyos lados están orientados hacia arriba y hacia la derecha, lo cual facilita el cálculo del área.\n",
        "\n",
        "* Función calcular_poligono: Para calcular el área del polígono, se utiliza el método de Gauss, también conocido como el método de trapecio. Este método integra la forma general de un polígono inscrito en un círculo unitario. La fórmula básica es:\n",
        "\n",
        "$[ \\text{Área} = \\frac{1}{2} \\left| \\sum_{i=0}^{n-1} (x_i y_{i+1} - x_{i+1} y_i) \\right| ]$\n",
        "\n",
        "donde $(x_i, y_i)$ son las coordenadas de los vértices del polígono, y la suma se realiza sobre todos los vértices, con el último vértice $(x_n, y_n)$ conectándose al primero $(x_0, y_0)$ para cerrar el polígono. La expresión dentro de los paréntesis calcula el valor absoluto de la suma de productos cruzados de las coordenadas x e y de cada par de vértices adyacentes, y luego se divide por 2 para obtener el área.\n",
        "\n",
        "* Función mostrar_resultado: Finalmente, esta función muestra el resultado del cálculo del área del polígono, indicando cuántos vértices tiene el polígono y cuál es su área.\n"
      ],
      "metadata": {
        "id": "0lT_tk9EwR3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWatnG9RtWTf",
        "outputId": "5f17c710-2ab4-4351-ae8c-c2b3f13326bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingrese la cantidad de vértices del polígono: 14\n",
            "Ingrese las coordenadas del vértice 1 en el formato 'x y': \n",
            "1 -1\n",
            "Ingrese las coordenadas del vértice 2 en el formato 'x y': \n",
            "1.9 2.5\n",
            "Ingrese las coordenadas del vértice 3 en el formato 'x y': \n",
            "7.5 8\n",
            "Ingrese las coordenadas del vértice 4 en el formato 'x y': \n",
            "15 9\n",
            "Ingrese las coordenadas del vértice 5 en el formato 'x y': \n",
            "10 9\n",
            "Ingrese las coordenadas del vértice 6 en el formato 'x y': \n",
            "8 7\n",
            "Ingrese las coordenadas del vértice 7 en el formato 'x y': \n",
            "5 4\n",
            "Ingrese las coordenadas del vértice 8 en el formato 'x y': \n",
            "3 2\n",
            "Ingrese las coordenadas del vértice 9 en el formato 'x y': \n",
            "2 1\n",
            "Ingrese las coordenadas del vértice 10 en el formato 'x y': \n",
            "9 7\n",
            "Ingrese las coordenadas del vértice 11 en el formato 'x y': \n",
            "18 15\n",
            "Ingrese las coordenadas del vértice 12 en el formato 'x y': \n",
            "14 -13.5\n",
            "Ingrese las coordenadas del vértice 13 en el formato 'x y': \n",
            "15.9 18\n",
            "Ingrese las coordenadas del vértice 14 en el formato 'x y': \n",
            "11 123\n",
            "El área del polígono con 14 vértices es: 510.09999999999997\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        num_vertices = int(input(\"Ingrese la cantidad de vértices del polígono: \"))\n",
        "        if num_vertices < 3:\n",
        "            print(\"El número de vértices debe ser igual o superior a 3.\")\n",
        "        else:\n",
        "            vertices = ingresar_vertices(num_vertices)\n",
        "            vertices = ordenar_sentido_contrario(vertices)\n",
        "            calcular_poligono(vertices)\n",
        "    except ValueError:\n",
        "        print(\"Entrada inválida. Por favor, ingrese un número válido.\")\n",
        "\n",
        "def ordenar_sentido_contrario(vertices):\n",
        "\n",
        "    ref_vertex = min(vertices, key=lambda x: (x[1], x[0]))\n",
        "\n",
        "    vertices.sort(key=lambda x: (np.arctan2(x[1] - ref_vertex[1], x[0] - ref_vertex[0]), -x[1], x[0]))\n",
        "    return vertices\n",
        "\n",
        "def ingresar_vertices(num_vertices):\n",
        "    vertices = []\n",
        "    for i in range(num_vertices):\n",
        "        while True:\n",
        "            try:\n",
        "                print(\"Ingrese las coordenadas del vértice\", i + 1, \"en el formato 'x y': \")\n",
        "                x, y = map(float, input().split())\n",
        "                vertices.append((x, y))\n",
        "                break\n",
        "            except ValueError:\n",
        "                print(\"Entrada inválida. Por favor, ingrese dos números separados por un espacio.\")\n",
        "    return vertices\n",
        "\n",
        "def calcular_poligono(vertices):\n",
        "    area = 0\n",
        "    for i in range(len(vertices)):\n",
        "        x1, y1 = vertices[i]\n",
        "        x2, y2 = vertices[(i + 1) % len(vertices)]\n",
        "        area += x1 * y2 - x2 * y1\n",
        "    area = abs(area) / 2\n",
        "    mostrar_resultado(area, len(vertices))\n",
        "\n",
        "def mostrar_resultado(area, num_vertices):\n",
        "    print(\"El área del polígono con\", num_vertices, \"vértices es:\", area)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Redes Neuronales**\n",
        "\n",
        "Las redes neuronales, inspiradas en el funcionamiento del sistema nervioso humano, son modelos matemáticos que imitan el procesamiento de información realizado por el cerebro. Están compuestas por unidades de procesamiento, denominadas neuronas, que se organizan en capas. La arquitectura típica de una red neuronal incluye una capa de entrada, una o más capas ocultas y una capa de salida. Las neuronas se conectan entre sí mediante pesos o ponderaciones, que representan la fuerza de conexión entre ellas. Los datos de entrada se presentan en la capa de entrada, y a través de un proceso de propagación, los valores se transmiten desde cada neurona a la siguiente, hasta llegar a la capa de salida, donde se genera el resultado final.\n",
        "\n",
        "Las redes neuronales utilizan funciones de activación para transformar las entradas ponderadas de una neurona en su activación a la salida. Estas funciones de activación introducen no linealidades en el modelo, permitiendo que la red aprenda relaciones complejas entre las características de entrada y la salida deseada. Ejemplos comunes de funciones de activación incluyen la función sigmoide y la tangente hiperbólica, aunque en la práctica moderna, la función ReLU (Rectified Linear Unit) es muy utilizada debido a su simplicidad y eficiencia computacional.\n",
        "\n",
        "El funcionamiento de las redes neuronales se basa en la capacidad de aprender de los errores, adaptándose y mejorando continuamente sus predicciones. Este proceso de aprendizaje profundo permite a las redes neuronales resolver problemas complejos con una precisión considerablemente alta, imitando de alguna manera el procesamiento de información del cerebro humano."
      ],
      "metadata": {
        "id": "57L1fqyUBp3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Capas Densas**\n",
        "Para las capas densas, los parámetros clave son:\n",
        "\n",
        "* Número de Neuronas: El número de neuronas en una capa densa determina cuántas características independientes puede aprender el modelo. Un número mayor de neuronas permite al modelo capturar relaciones más complejas en los datos, pero también aumenta el riesgo de sobreajuste y el costo computacional. La elección del número óptimo de neuronas suele hacerse experimentalmente, probando diferentes configuraciones y eligiendo aquella que ofrece el mejor rendimiento en un conjunto de validación.\n",
        "\n",
        "* Función de Activación: La función de activación introduce no linealidades en el modelo, permitiendo que aprenda relaciones complejas entre las características. La función ReLU (Rectified Linear Unit) es comúnmente usada debido a su simplicidad y eficiencia computacional. Otras opciones incluyen la función sigmoidea y la tangente hiperbólica, aunque estas últimas tienden a ser menos populares en modelos modernos debido a su menor eficiencia.\n",
        "\n",
        "* Regularización: La regularización es una técnica para prevenir el sobreajuste, añadiendo un término al objetivo de coste que penaliza grandes pesos. La regularización L2, que penaliza directamente los pesos, es una opción popular. La elección de la constante de regularización (por ejemplo, 0.01 en el código) es crítica y suele requerir experimentación.\n",
        "\n",
        "# **Capas de Dropout**\n",
        "Para las capas de dropout, el parámetro clave es:\n",
        "\n",
        "* Tasa de Dropout: La tasa de dropout determina qué proporción de las neuronas se \"desactivará\" (es decir, se ignorará temporalmente durante el entrenamiento) en cada paso. Una alta tasa de dropout puede ayudar a prevenir el sobreajuste al obligar al modelo a aprender representaciones más robustas, ya que no puede confiar en ninguna neurona individual. Sin embargo, si la tasa de dropout es demasialta, el modelo podría no aprender adecuadamente. Una tasa comúnmente usada es 0.5, pero esto puede variar según el problema y requiere experimentación para encontrar el valor óptimo."
      ],
      "metadata": {
        "id": "Ml_0Sf0wE9AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explicación de las redes neuronales en el Código**\n",
        "* Entrada y Procesamiento de Datos:\n",
        "  1. Datos de Entrada: Los datos de entrada son las coordenadas de los vértices de los polígonos, que se aplanan en una matriz lineal antes de ser alimentados a la red neuronal. Esta matriz representa las características de cada polígono.\n",
        "\n",
        "* Arquitectura de la Red Neuronal:\n",
        "  1. Capas Densas: La red neuronal se construye utilizando capas densas (Dense). La capa de entrada tiene tantas neuronas como características (coordenadas de los vértices aplanadas). Las siguientes capas ocultas contienen neuronas que aprenden representaciones más abstractas de los datos de entrada. La última capa es la capa de salida, que produce una única salida: la predicción del área del polígono.\n",
        "  2. Funciones de Activación: Se utiliza la función de activación ReLU (Rectified Linear Unit) en las capas ocultas para introducir no linealidades en el modelo, permitiendo que la red aprenda relaciones complejas entre las características de entrada y la salida deseada.\n",
        "  3. Regularización y Dropout: Para prevenir el sobreajuste, se aplica regularización L2 a las capas densas y se utilizan capas de dropout. La regularización L2 penaliza los pesos grandes, favoreciendo soluciones más simples y estables. El dropout \"apaga\" aleatoriamente algunas neuronas durante el entrenamiento, lo que ayuda a prevenir el sobreajuste al hacer que el modelo sea menos dependiente de cualquier neurona individual.\n",
        "\n",
        "* Entrenamiento y Aprendizaje\n",
        "  1. Entrenamiento: Durante el entrenamiento, la red neuronal ajusta sus pesos para minimizar la pérdida, que en este caso es la media cuadrática del error (MSE) entre las predicciones de la red y los valores reales de las áreas de los polígonos. El optimizador utilizado es Adam, que es eficiente y efectivo para muchos tipos de problemas de aprendizaje profundo.\n",
        "  2. Retropropagación: El proceso de retropropagación es esencial para el aprendizaje de la red. Cuando la red emite una predicción incorrecta, este error se propaga hacia atrás a través de la red, ajustando los pesos de manera que futuras predicciones sean más precisas.\n",
        "\n",
        "* Uso y Predicciones\n",
        "  1. Predicción de Áreas: Una vez entrenada, la red neuronal puede tomar nuevas entradas (coordenadas de vértices de polígonos) y predecir su área basándose en lo que ha aprendido durante el entrenamiento. Esto demuestra la capacidad de las redes neuronales para generalizar de los datos de entrenamiento a nuevos datos sin haber sido previamente expuestos."
      ],
      "metadata": {
        "id": "s9s0szamDGOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "def generate_polygon_data(num_samples, max_vertices=10):\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        num_vertices = random.randint(3, max_vertices)\n",
        "        vertices = [(random.uniform(-10, 10), random.uniform(-10, 10)) for _ in range(num_vertices)]\n",
        "        area = calculate_polygon_area(vertices)\n",
        "        flat_vertices = [coord for vertex in vertices for coord in vertex]\n",
        "        flat_vertices += [0] * (max_vertices * 2 - len(flat_vertices))\n",
        "        data.append((flat_vertices, area))\n",
        "    return data\n",
        "\n",
        "def calculate_polygon_area(vertices):\n",
        "    area = 0\n",
        "    n = len(vertices)\n",
        "    for i in range(n):\n",
        "        x1, y1 = vertices[i]\n",
        "        x2, y2 = vertices[(i + 1) % n]\n",
        "        area += x1 * y2 - x2 * y1\n",
        "    return abs(area) / 2.0\n",
        "\n",
        "\n",
        "data = generate_polygon_data(100000)\n",
        "X = np.array([item[0] for item in data])\n",
        "y = np.array([item[1] for item in data])\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, epochs=250, batch_size=32, validation_split=0.2)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "example_polygon = [(1, 1), (4, 1), (4, 5), (1, 5)]\n",
        "flat_example = [coord for vertex in example_polygon for coord in vertex]\n",
        "flat_example += [0] * (20 - len(flat_example))\n",
        "flat_example = scaler.transform([flat_example])\n",
        "predicted_area = model.predict(flat_example)[0][0]\n",
        "actual_area = calculate_polygon_area(example_polygon)\n",
        "print(f\"Predicted Area: {predicted_area}, Actual Area: {actual_area}\")\n",
        "\n",
        "model.save('polygon_area_model.h5')\n",
        "\n",
        "model = load_model('polygon_area_model.h5')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error after further training: {mse}\")\n",
        "\n",
        "predicted_area = model.predict(flat_example)[0][0]\n",
        "print(f\"Predicted Area after further training: {predicted_area}, Actual Area: {actual_area}\")\n",
        "\n",
        "model.save('polygon_area_model_updated.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xihCOrru0OUE",
        "outputId": "aeb3a756-dbe4-4629-a682-ba5f692bc3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "2000/2000 [==============================] - 8s 3ms/step - loss: 1362.1044 - val_loss: 1196.0798\n",
            "Epoch 2/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1261.6195 - val_loss: 1167.9431\n",
            "Epoch 3/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1236.7544 - val_loss: 1150.6633\n",
            "Epoch 4/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1228.0432 - val_loss: 1147.3063\n",
            "Epoch 5/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1226.6887 - val_loss: 1140.5055\n",
            "Epoch 6/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1219.2296 - val_loss: 1146.1920\n",
            "Epoch 7/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1214.2081 - val_loss: 1148.7528\n",
            "Epoch 8/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1214.1782 - val_loss: 1155.4329\n",
            "Epoch 9/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1215.4402 - val_loss: 1177.4939\n",
            "Epoch 10/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1215.5432 - val_loss: 1146.3726\n",
            "Epoch 11/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1216.8964 - val_loss: 1157.7310\n",
            "Epoch 12/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 1213.3055 - val_loss: 1161.7617\n",
            "Epoch 13/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1208.4330 - val_loss: 1142.3037\n",
            "Epoch 14/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1207.3049 - val_loss: 1146.3384\n",
            "Epoch 15/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1208.2803 - val_loss: 1153.8475\n",
            "Epoch 16/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1203.3374 - val_loss: 1147.6283\n",
            "Epoch 17/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1207.2408 - val_loss: 1157.5385\n",
            "Epoch 18/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1205.3020 - val_loss: 1146.7755\n",
            "Epoch 19/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1199.2301 - val_loss: 1148.2584\n",
            "Epoch 20/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1197.8998 - val_loss: 1133.8285\n",
            "Epoch 21/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1200.0211 - val_loss: 1176.8551\n",
            "Epoch 22/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1203.4240 - val_loss: 1142.7401\n",
            "Epoch 23/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 1200.3580 - val_loss: 1139.7886\n",
            "Epoch 24/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1200.8315 - val_loss: 1144.9344\n",
            "Epoch 25/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1193.2360 - val_loss: 1134.8198\n",
            "Epoch 26/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 1203.1985 - val_loss: 1143.1066\n",
            "Epoch 27/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1197.6543 - val_loss: 1142.2853\n",
            "Epoch 28/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1194.0098 - val_loss: 1154.0256\n",
            "Epoch 29/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 1198.5444 - val_loss: 1164.3237\n",
            "Epoch 30/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1191.7172 - val_loss: 1157.6315\n",
            "Epoch 31/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1189.2871 - val_loss: 1138.0614\n",
            "Epoch 32/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1196.2407 - val_loss: 1179.9078\n",
            "Epoch 33/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1193.0071 - val_loss: 1135.5052\n",
            "Epoch 34/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1185.5798 - val_loss: 1121.9003\n",
            "Epoch 35/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1171.6166 - val_loss: 1104.6121\n",
            "Epoch 36/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1159.9998 - val_loss: 1092.5513\n",
            "Epoch 37/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1148.1718 - val_loss: 1070.7206\n",
            "Epoch 38/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1142.6621 - val_loss: 1107.3596\n",
            "Epoch 39/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1141.5100 - val_loss: 1090.6418\n",
            "Epoch 40/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1138.7281 - val_loss: 1074.5243\n",
            "Epoch 41/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1132.2053 - val_loss: 1088.0037\n",
            "Epoch 42/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1134.2827 - val_loss: 1069.8452\n",
            "Epoch 43/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1122.6542 - val_loss: 1078.6167\n",
            "Epoch 44/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1119.1171 - val_loss: 1053.0179\n",
            "Epoch 45/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 1121.7521 - val_loss: 1065.3409\n",
            "Epoch 46/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1116.2245 - val_loss: 1038.6815\n",
            "Epoch 47/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1112.2015 - val_loss: 1046.6508\n",
            "Epoch 48/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1112.4966 - val_loss: 1052.1786\n",
            "Epoch 49/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 1101.0771 - val_loss: 1089.6467\n",
            "Epoch 50/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1091.3301 - val_loss: 1025.9971\n",
            "Epoch 51/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 1073.9567 - val_loss: 996.9973\n",
            "Epoch 52/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 1041.9491 - val_loss: 901.0375\n",
            "Epoch 53/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 976.0674 - val_loss: 788.9014\n",
            "Epoch 54/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 915.1917 - val_loss: 659.2255\n",
            "Epoch 55/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 852.3564 - val_loss: 644.2840\n",
            "Epoch 56/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 821.1744 - val_loss: 559.1741\n",
            "Epoch 57/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 785.6432 - val_loss: 602.4293\n",
            "Epoch 58/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 763.4079 - val_loss: 537.6697\n",
            "Epoch 59/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 747.0446 - val_loss: 594.0024\n",
            "Epoch 60/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 725.4547 - val_loss: 584.8362\n",
            "Epoch 61/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 716.8420 - val_loss: 659.3224\n",
            "Epoch 62/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 701.1470 - val_loss: 666.2754\n",
            "Epoch 63/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 691.3063 - val_loss: 704.6346\n",
            "Epoch 64/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 688.8918 - val_loss: 694.4599\n",
            "Epoch 65/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 672.3464 - val_loss: 736.4067\n",
            "Epoch 66/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 667.8871 - val_loss: 756.6522\n",
            "Epoch 67/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 670.9199 - val_loss: 802.3611\n",
            "Epoch 68/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 659.2282 - val_loss: 827.4763\n",
            "Epoch 69/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 656.0168 - val_loss: 824.3353\n",
            "Epoch 70/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 656.3305 - val_loss: 906.5396\n",
            "Epoch 71/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 651.0294 - val_loss: 923.5448\n",
            "Epoch 72/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 644.4350 - val_loss: 855.3168\n",
            "Epoch 73/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 643.4614 - val_loss: 923.2242\n",
            "Epoch 74/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 643.9844 - val_loss: 956.9614\n",
            "Epoch 75/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 639.8022 - val_loss: 963.9586\n",
            "Epoch 76/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 635.9642 - val_loss: 1007.8310\n",
            "Epoch 77/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 634.7816 - val_loss: 997.7968\n",
            "Epoch 78/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 632.5238 - val_loss: 999.2822\n",
            "Epoch 79/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 630.6389 - val_loss: 979.2195\n",
            "Epoch 80/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 632.6362 - val_loss: 1029.4443\n",
            "Epoch 81/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 629.9861 - val_loss: 1085.1973\n",
            "Epoch 82/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 630.2851 - val_loss: 1038.1685\n",
            "Epoch 83/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 623.7822 - val_loss: 1064.3062\n",
            "Epoch 84/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 628.9213 - val_loss: 1136.7202\n",
            "Epoch 85/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 627.8898 - val_loss: 1103.3538\n",
            "Epoch 86/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 621.5962 - val_loss: 1108.2078\n",
            "Epoch 87/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 620.0439 - val_loss: 1118.7085\n",
            "Epoch 88/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 623.5968 - val_loss: 1148.9791\n",
            "Epoch 89/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 621.8469 - val_loss: 1154.5447\n",
            "Epoch 90/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 626.6721 - val_loss: 1147.5186\n",
            "Epoch 91/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 620.4498 - val_loss: 1207.0620\n",
            "Epoch 92/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 616.5921 - val_loss: 1178.1154\n",
            "Epoch 93/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 610.7516 - val_loss: 1169.3755\n",
            "Epoch 94/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 618.1841 - val_loss: 1204.8470\n",
            "Epoch 95/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 622.1918 - val_loss: 1206.4333\n",
            "Epoch 96/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 610.2192 - val_loss: 1199.6512\n",
            "Epoch 97/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 614.2104 - val_loss: 1180.6344\n",
            "Epoch 98/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 619.2518 - val_loss: 1198.4473\n",
            "Epoch 99/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 614.0956 - val_loss: 1203.5419\n",
            "Epoch 100/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 608.0938 - val_loss: 1190.2971\n",
            "Epoch 101/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 617.3524 - val_loss: 1208.4073\n",
            "Epoch 102/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 607.2107 - val_loss: 1230.4263\n",
            "Epoch 103/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 610.7693 - val_loss: 1245.1807\n",
            "Epoch 104/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 607.3052 - val_loss: 1191.1763\n",
            "Epoch 105/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 604.1123 - val_loss: 1277.9484\n",
            "Epoch 106/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 613.3690 - val_loss: 1195.7726\n",
            "Epoch 107/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 607.3700 - val_loss: 1255.9707\n",
            "Epoch 108/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 610.5184 - val_loss: 1222.5802\n",
            "Epoch 109/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 612.7488 - val_loss: 1242.9327\n",
            "Epoch 110/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 607.4808 - val_loss: 1225.9076\n",
            "Epoch 111/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 610.3465 - val_loss: 1288.9071\n",
            "Epoch 112/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 603.1746 - val_loss: 1298.9830\n",
            "Epoch 113/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 601.9265 - val_loss: 1184.5066\n",
            "Epoch 114/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 600.0165 - val_loss: 1283.9288\n",
            "Epoch 115/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 607.0408 - val_loss: 1274.4222\n",
            "Epoch 116/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 611.1198 - val_loss: 1321.2545\n",
            "Epoch 117/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 606.7350 - val_loss: 1186.1364\n",
            "Epoch 118/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 596.4902 - val_loss: 1259.3964\n",
            "Epoch 119/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 605.1111 - val_loss: 1263.8126\n",
            "Epoch 120/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 598.7819 - val_loss: 1298.0214\n",
            "Epoch 121/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 603.2766 - val_loss: 1320.9249\n",
            "Epoch 122/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 596.4974 - val_loss: 1258.5060\n",
            "Epoch 123/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 596.0620 - val_loss: 1322.6428\n",
            "Epoch 124/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 606.9109 - val_loss: 1317.1412\n",
            "Epoch 125/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 604.7817 - val_loss: 1296.4125\n",
            "Epoch 126/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 603.7347 - val_loss: 1288.6500\n",
            "Epoch 127/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 599.9286 - val_loss: 1294.9705\n",
            "Epoch 128/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 603.3583 - val_loss: 1257.0898\n",
            "Epoch 129/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 597.8631 - val_loss: 1342.4695\n",
            "Epoch 130/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 594.4938 - val_loss: 1358.9823\n",
            "Epoch 131/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 593.2551 - val_loss: 1294.1808\n",
            "Epoch 132/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 596.4601 - val_loss: 1379.2131\n",
            "Epoch 133/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 599.5255 - val_loss: 1318.5813\n",
            "Epoch 134/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 598.9691 - val_loss: 1320.2538\n",
            "Epoch 135/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 595.7827 - val_loss: 1291.1965\n",
            "Epoch 136/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 593.7061 - val_loss: 1407.5726\n",
            "Epoch 137/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 598.4186 - val_loss: 1390.9790\n",
            "Epoch 138/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 596.3446 - val_loss: 1293.0625\n",
            "Epoch 139/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 597.1028 - val_loss: 1288.3115\n",
            "Epoch 140/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 601.2903 - val_loss: 1397.8623\n",
            "Epoch 141/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 593.7447 - val_loss: 1375.2461\n",
            "Epoch 142/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 600.1057 - val_loss: 1346.8486\n",
            "Epoch 143/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 593.5131 - val_loss: 1342.1605\n",
            "Epoch 144/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 595.8392 - val_loss: 1444.0482\n",
            "Epoch 145/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 599.9737 - val_loss: 1414.2904\n",
            "Epoch 146/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 596.5705 - val_loss: 1339.3284\n",
            "Epoch 147/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 594.4949 - val_loss: 1368.3389\n",
            "Epoch 148/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 600.2973 - val_loss: 1284.5629\n",
            "Epoch 149/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 596.5729 - val_loss: 1355.7212\n",
            "Epoch 150/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 596.1284 - val_loss: 1396.4469\n",
            "Epoch 151/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 589.4755 - val_loss: 1355.8113\n",
            "Epoch 152/250\n",
            "2000/2000 [==============================] - 10s 5ms/step - loss: 595.5050 - val_loss: 1366.8055\n",
            "Epoch 153/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 595.8469 - val_loss: 1345.8181\n",
            "Epoch 154/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 597.4484 - val_loss: 1407.1843\n",
            "Epoch 155/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 593.9900 - val_loss: 1384.4497\n",
            "Epoch 156/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 588.1248 - val_loss: 1361.1871\n",
            "Epoch 157/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 597.3522 - val_loss: 1344.2397\n",
            "Epoch 158/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 593.6212 - val_loss: 1431.6284\n",
            "Epoch 159/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 595.6149 - val_loss: 1379.5762\n",
            "Epoch 160/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 587.6791 - val_loss: 1373.9886\n",
            "Epoch 161/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 590.3179 - val_loss: 1413.2993\n",
            "Epoch 162/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 590.3831 - val_loss: 1467.8789\n",
            "Epoch 163/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 591.8025 - val_loss: 1427.1748\n",
            "Epoch 164/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 590.3938 - val_loss: 1410.2083\n",
            "Epoch 165/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 590.3890 - val_loss: 1424.1216\n",
            "Epoch 166/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 587.6521 - val_loss: 1366.0209\n",
            "Epoch 167/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 594.4738 - val_loss: 1439.9698\n",
            "Epoch 168/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 587.3915 - val_loss: 1446.5148\n",
            "Epoch 169/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 594.7356 - val_loss: 1385.7823\n",
            "Epoch 170/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 593.3666 - val_loss: 1415.3630\n",
            "Epoch 171/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 590.6542 - val_loss: 1395.2310\n",
            "Epoch 172/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 588.1556 - val_loss: 1439.4272\n",
            "Epoch 173/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 586.2115 - val_loss: 1412.5214\n",
            "Epoch 174/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 590.1924 - val_loss: 1408.1581\n",
            "Epoch 175/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 584.0842 - val_loss: 1441.0808\n",
            "Epoch 176/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 586.8344 - val_loss: 1395.0831\n",
            "Epoch 177/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 587.9165 - val_loss: 1444.8147\n",
            "Epoch 178/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 588.5395 - val_loss: 1361.6215\n",
            "Epoch 179/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 584.8149 - val_loss: 1417.2668\n",
            "Epoch 180/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 592.1785 - val_loss: 1488.6885\n",
            "Epoch 181/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 592.1911 - val_loss: 1463.9043\n",
            "Epoch 182/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 587.4431 - val_loss: 1422.7064\n",
            "Epoch 183/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 590.8851 - val_loss: 1416.4198\n",
            "Epoch 184/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 591.2318 - val_loss: 1449.8698\n",
            "Epoch 185/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 589.0731 - val_loss: 1463.4966\n",
            "Epoch 186/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 590.2315 - val_loss: 1425.9377\n",
            "Epoch 187/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 588.8077 - val_loss: 1418.6873\n",
            "Epoch 188/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 588.6859 - val_loss: 1447.5153\n",
            "Epoch 189/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 594.7413 - val_loss: 1387.9614\n",
            "Epoch 190/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 584.5796 - val_loss: 1423.7211\n",
            "Epoch 191/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 587.7136 - val_loss: 1377.7438\n",
            "Epoch 192/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 585.0693 - val_loss: 1412.2870\n",
            "Epoch 193/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 587.0539 - val_loss: 1411.4758\n",
            "Epoch 194/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 588.2912 - val_loss: 1368.2900\n",
            "Epoch 195/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 588.7201 - val_loss: 1373.2517\n",
            "Epoch 196/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 580.0931 - val_loss: 1415.0029\n",
            "Epoch 197/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 584.7503 - val_loss: 1407.2239\n",
            "Epoch 198/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 581.7581 - val_loss: 1514.9484\n",
            "Epoch 199/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 587.9460 - val_loss: 1461.0538\n",
            "Epoch 200/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 584.7897 - val_loss: 1399.4039\n",
            "Epoch 201/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 583.0126 - val_loss: 1406.1884\n",
            "Epoch 202/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 587.7517 - val_loss: 1400.9280\n",
            "Epoch 203/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 589.7838 - val_loss: 1446.4358\n",
            "Epoch 204/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 586.5609 - val_loss: 1369.6796\n",
            "Epoch 205/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 585.7301 - val_loss: 1458.2715\n",
            "Epoch 206/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 586.5025 - val_loss: 1432.4214\n",
            "Epoch 207/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 588.2595 - val_loss: 1374.2922\n",
            "Epoch 208/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 583.8627 - val_loss: 1497.5712\n",
            "Epoch 209/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 582.9274 - val_loss: 1504.2621\n",
            "Epoch 210/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 583.6448 - val_loss: 1424.7386\n",
            "Epoch 211/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 585.8795 - val_loss: 1470.8115\n",
            "Epoch 212/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 587.2048 - val_loss: 1411.4860\n",
            "Epoch 213/250\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 578.1291 - val_loss: 1467.0828\n",
            "Epoch 214/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 581.9135 - val_loss: 1446.3032\n",
            "Epoch 215/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 575.9573 - val_loss: 1450.9089\n",
            "Epoch 216/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 580.1589 - val_loss: 1421.8654\n",
            "Epoch 217/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.5861 - val_loss: 1505.1870\n",
            "Epoch 218/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 581.1885 - val_loss: 1446.8737\n",
            "Epoch 219/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 580.5535 - val_loss: 1412.2704\n",
            "Epoch 220/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 575.2315 - val_loss: 1463.5369\n",
            "Epoch 221/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 578.6674 - val_loss: 1443.1115\n",
            "Epoch 222/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 581.5922 - val_loss: 1520.2638\n",
            "Epoch 223/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 579.4686 - val_loss: 1496.8080\n",
            "Epoch 224/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 579.0588 - val_loss: 1459.4261\n",
            "Epoch 225/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 582.4863 - val_loss: 1455.6455\n",
            "Epoch 226/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 577.7933 - val_loss: 1430.4288\n",
            "Epoch 227/250\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 581.8168 - val_loss: 1415.9801\n",
            "Epoch 228/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 584.6961 - val_loss: 1474.6075\n",
            "Epoch 229/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 574.9446 - val_loss: 1499.7301\n",
            "Epoch 230/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 580.3596 - val_loss: 1484.2900\n",
            "Epoch 231/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 580.6144 - val_loss: 1448.0754\n",
            "Epoch 232/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 580.7028 - val_loss: 1443.1361\n",
            "Epoch 233/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 585.0055 - val_loss: 1509.8138\n",
            "Epoch 234/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 583.5779 - val_loss: 1382.6061\n",
            "Epoch 235/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 582.6098 - val_loss: 1456.2534\n",
            "Epoch 236/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 585.5494 - val_loss: 1463.4038\n",
            "Epoch 237/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 582.2800 - val_loss: 1435.7917\n",
            "Epoch 238/250\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 582.1730 - val_loss: 1465.5079\n",
            "Epoch 239/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 577.7429 - val_loss: 1517.5953\n",
            "Epoch 240/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 583.9557 - val_loss: 1430.9270\n",
            "Epoch 241/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 584.0247 - val_loss: 1481.4371\n",
            "Epoch 242/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 575.5840 - val_loss: 1491.4360\n",
            "Epoch 243/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 583.0017 - val_loss: 1444.3444\n",
            "Epoch 244/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 579.7229 - val_loss: 1485.6223\n",
            "Epoch 245/250\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 575.9774 - val_loss: 1430.0277\n",
            "Epoch 246/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.7648 - val_loss: 1461.7777\n",
            "Epoch 247/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.7166 - val_loss: 1413.2024\n",
            "Epoch 248/250\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 578.5728 - val_loss: 1438.3329\n",
            "Epoch 249/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 579.8726 - val_loss: 1445.3634\n",
            "Epoch 250/250\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.4037 - val_loss: 1479.8409\n",
            "625/625 [==============================] - 1s 2ms/step\n",
            "Mean Squared Error: 1484.6871716124\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "Predicted Area: 11.848877906799316, Actual Area: 12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 579.3201 - val_loss: 1440.4475\n",
            "Epoch 2/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 583.1365 - val_loss: 1382.4841\n",
            "Epoch 3/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 583.3057 - val_loss: 1520.0350\n",
            "Epoch 4/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 576.1986 - val_loss: 1461.6902\n",
            "Epoch 5/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 577.0291 - val_loss: 1479.3602\n",
            "Epoch 6/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 571.4273 - val_loss: 1420.1312\n",
            "Epoch 7/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.5116 - val_loss: 1513.1968\n",
            "Epoch 8/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 577.4791 - val_loss: 1425.6388\n",
            "Epoch 9/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.0145 - val_loss: 1441.3834\n",
            "Epoch 10/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.0803 - val_loss: 1507.5809\n",
            "Epoch 11/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 576.0730 - val_loss: 1442.8234\n",
            "Epoch 12/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 573.4519 - val_loss: 1508.8910\n",
            "Epoch 13/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 574.2769 - val_loss: 1543.2780\n",
            "Epoch 14/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 579.0554 - val_loss: 1473.6495\n",
            "Epoch 15/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 577.5685 - val_loss: 1506.9515\n",
            "Epoch 16/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.6848 - val_loss: 1448.0092\n",
            "Epoch 17/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 575.5442 - val_loss: 1479.6715\n",
            "Epoch 18/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.7938 - val_loss: 1491.4426\n",
            "Epoch 19/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 576.8117 - val_loss: 1408.2618\n",
            "Epoch 20/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 576.7159 - val_loss: 1559.8506\n",
            "Epoch 21/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 575.6363 - val_loss: 1516.0159\n",
            "Epoch 22/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 582.1925 - val_loss: 1484.0055\n",
            "Epoch 23/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 577.9646 - val_loss: 1517.5129\n",
            "Epoch 24/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 576.2776 - val_loss: 1517.4523\n",
            "Epoch 25/50\n",
            "2000/2000 [==============================] - 9s 5ms/step - loss: 576.4866 - val_loss: 1492.4868\n",
            "Epoch 26/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 575.4601 - val_loss: 1421.9578\n",
            "Epoch 27/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.6229 - val_loss: 1461.6995\n",
            "Epoch 28/50\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 572.6616 - val_loss: 1409.1724\n",
            "Epoch 29/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 574.7712 - val_loss: 1534.6394\n",
            "Epoch 30/50\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 578.2950 - val_loss: 1534.4325\n",
            "Epoch 31/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 575.9940 - val_loss: 1546.4487\n",
            "Epoch 32/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 575.1284 - val_loss: 1481.3811\n",
            "Epoch 33/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.0696 - val_loss: 1459.2754\n",
            "Epoch 34/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 582.5756 - val_loss: 1437.3015\n",
            "Epoch 35/50\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 571.5890 - val_loss: 1480.0591\n",
            "Epoch 36/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.1508 - val_loss: 1458.1636\n",
            "Epoch 37/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 577.3693 - val_loss: 1510.2043\n",
            "Epoch 38/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 575.8592 - val_loss: 1387.1536\n",
            "Epoch 39/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 582.1482 - val_loss: 1578.8420\n",
            "Epoch 40/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 577.9069 - val_loss: 1534.2421\n",
            "Epoch 41/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 574.1822 - val_loss: 1507.0580\n",
            "Epoch 42/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 578.6526 - val_loss: 1513.5514\n",
            "Epoch 43/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 571.8098 - val_loss: 1473.9904\n",
            "Epoch 44/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 573.8156 - val_loss: 1408.4501\n",
            "Epoch 45/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 573.6772 - val_loss: 1468.3165\n",
            "Epoch 46/50\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 574.6216 - val_loss: 1431.2322\n",
            "Epoch 47/50\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 575.4514 - val_loss: 1401.5361\n",
            "Epoch 48/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 569.6443 - val_loss: 1547.5974\n",
            "Epoch 49/50\n",
            "2000/2000 [==============================] - 9s 4ms/step - loss: 579.8630 - val_loss: 1496.5477\n",
            "Epoch 50/50\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 571.1303 - val_loss: 1485.2946\n",
            "625/625 [==============================] - 1s 2ms/step\n",
            "Mean Squared Error after further training: 1484.1732353286673\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "Predicted Area after further training: 10.89993667602539, Actual Area: 12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('polygon_area_model_updated.h5')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.2)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error after further training: {mse}\")\n",
        "\n",
        "predicted_area = model.predict(flat_example)[0][0]\n",
        "print(f\"Predicted Area after further training: {predicted_area}, Actual Area: {actual_area}\")\n",
        "\n",
        "model.save('polygon_area_model_updated.h6')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59UwNNT4Go7T",
        "outputId": "5ca3ac0f-ad70-43b3-f1bc-247a077e4e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 582.0919 - val_loss: 1512.5107\n",
            "Epoch 2/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 579.1697 - val_loss: 1451.8031\n",
            "Epoch 3/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 575.8791 - val_loss: 1414.0615\n",
            "Epoch 4/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 575.4818 - val_loss: 1456.3690\n",
            "Epoch 5/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 574.0463 - val_loss: 1386.5636\n",
            "Epoch 6/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 575.2607 - val_loss: 1443.4403\n",
            "Epoch 7/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 571.6216 - val_loss: 1583.8409\n",
            "Epoch 8/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 568.0396 - val_loss: 1545.1704\n",
            "Epoch 9/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 573.1001 - val_loss: 1436.6185\n",
            "Epoch 10/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 576.4539 - val_loss: 1550.5663\n",
            "Epoch 11/25\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 576.2032 - val_loss: 1547.1779\n",
            "Epoch 12/25\n",
            "2000/2000 [==============================] - 7s 4ms/step - loss: 575.9587 - val_loss: 1428.0258\n",
            "Epoch 13/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 575.1049 - val_loss: 1510.8391\n",
            "Epoch 14/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 574.3934 - val_loss: 1400.4199\n",
            "Epoch 15/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 574.7506 - val_loss: 1465.4830\n",
            "Epoch 16/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 576.5718 - val_loss: 1440.6019\n",
            "Epoch 17/25\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 570.6268 - val_loss: 1428.1726\n",
            "Epoch 18/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 574.7665 - val_loss: 1494.7892\n",
            "Epoch 19/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 573.3455 - val_loss: 1478.4829\n",
            "Epoch 20/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 569.6212 - val_loss: 1488.5043\n",
            "Epoch 21/25\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 571.9781 - val_loss: 1545.6653\n",
            "Epoch 22/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 572.1897 - val_loss: 1469.3188\n",
            "Epoch 23/25\n",
            "2000/2000 [==============================] - 6s 3ms/step - loss: 567.8501 - val_loss: 1542.7465\n",
            "Epoch 24/25\n",
            "2000/2000 [==============================] - 8s 4ms/step - loss: 574.0073 - val_loss: 1475.4050\n",
            "Epoch 25/25\n",
            "2000/2000 [==============================] - 7s 3ms/step - loss: 567.6373 - val_loss: 1523.1300\n",
            "625/625 [==============================] - 1s 2ms/step\n",
            "Mean Squared Error after further training: 1525.63437080211\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "Predicted Area after further training: 11.059188842773438, Actual Area: 12.0\n"
          ]
        }
      ]
    }
  ]
}